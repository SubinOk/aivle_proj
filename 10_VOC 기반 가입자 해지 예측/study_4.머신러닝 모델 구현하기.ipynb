{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 안녕하세요^^ \n","# AIVLE 미니 프로젝트에 오신 여러분을 환영합니다.\n","* 본 과정에서는 실제 사례와 데이터를 기반으로 문제를 해결하는 전체 과정을 자기 주도형 실습으로 진행해볼 예정입니다.\n","* 앞선 교육과정을 정리하는 마음과 지금까지 배운 내용을 바탕으로 문제 해결을 해볼게요!\n","* 미니 프로젝트를 통한 문제 해결 과정 'A에서 Z까지', 지금부터 시작합니다!\n","---\n","## __VOC(Voice of Customer) 데이터를 활용한 가입자 해지 예측 모델링__\n",">#### 1. AIDU 기본 사용법 및 데이터 분석 준비하기\n",">#### 2. 데이터 탐색하기\n",">#### 3. 데이터 전처리하기\n",">### __4. 머신러닝 모델 구현__\n",">#### 5. 딥러닝 심층신경망 모델 구현"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 코드실행시 경고 메시지 무시\n","import warnings\n","warnings.filterwarnings(action='ignore') "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !pip install seaborn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 필요 라이브러리 임포트 하기\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["## __Chapter 4. 머신러닝 모델 구현__\n","> 1. 데이터 가져오기\n","> 1. 데이터 전처리 (완료)\n","> 1. Train, Test 데이터셋 분할\n","> 1. 데이터 정규화 (MinMaxScaling, StandardScaling)\n","> 1. 머신러닝 모델 구현<br>\n","> 1. 머신러닝 성능 평가<br>\n","\n","### __1. 데이터 가져오기__\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ROOT_PATH 확인 \n","import os\n","\n","WORK_SPACE = \"\"\n","\n","if os.getcwd() == '/content' :\n","  # 구글 드라이브 사용 시 \n","  ROOT_PATH = \"/content\" \n","else :\n","  ROOT_PATH = os.path.abspath('..')\n","\n","DATA_PATH = ROOT_PATH + '/data'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# voc_rcp_practice3.csv  데이터 읽어 들이기\n","df = pd.read_csv(DATA_PATH + \"/voc_rcp_practice3.csv\")\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 80컬럼, 20953 라인\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['trm_yn'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### __2. 데이터 전처리하기 - 완료__\n","\n","### __3. Train, Test  데이터셋 분할__\n","> ① Feature / Target 데이터 분리<br>\n","> ② Train / Test 데이터 Set 나누기\n","\n","\n","#### __① Feature / Target 데이터 분리__\n","- Feature는 X, Target은 y로 분리\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Feature는 Target('trm_yn') 제외한 나머지 \n","X = df.drop(columns=['trm_yn']).values\n","\n","# Target은 해지여부('trm_yn')\n","y = df['trm_yn'].values"]},{"cell_type":"markdown","metadata":{},"source":["\n","#### __② Train / Test 데이터 Set 나누기__\n","- X, y 값을 가지고 7:3 비율로 Train, Test 을 나누세요.<br>\n","- train_test_split 시 stratify 옵션 사용 <br>\n","\n","<img src=\"https://cdn.shortpixel.ai/spai/q_lossy+w_730+to_webp+ret_img/https://algotrading101.com/learn/wp-content/uploads/2020/06/training-validation-test-data-set.png\" width=600> <br>\n","\n","[출처] https://algotrading101.com/learn/train-test-split/\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(X.shape, y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 라이브러리 임포트\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 데이터 Set 나누기 (Train:Test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.3, stratify=y, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(X_train.shape, y_train.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### __4. 데이터 스케일링(표준화, 정규화)__\n","입력 변수(Feature)들의 값을 일정한 수준으로 맞춰 주는 것은 Feature Scalining 이라 합니다.<br> \n","데이터 스케일링의 목적은 컬럼 별 차이를 왜곡하지 않고 공통 척도로 변경하기 위함입니다. <br>\n","가령 아래처럼 나이의 범위는 (0 ~ 100) 이고 , 소득의 범위는 (0 ~ 10,000,000) 이라고 하면 소득은 나이의 약 100,000배이며 범위의 Range도 넓습니다.<br> \n","이 데이터를 그대로 사용하면 소득을 본질적으로 더 큰 값이기 때문에 나이 피쳐보다 더 큰 영향을 미치게 됩니다.\n","\n","<img src=\"https://ashutoshtripathicom.files.wordpress.com/2021/06/image-2.png\" width=800> <br>\n","[출처] https://ashutoshtripathi.com/2021/06/12/what-is-feature-scaling-in-machine-learning-normalization-vs-standardization/\n","\n","\n","#### __① Standard Scaler (표준화)__\n","<img src=\"https://ashutoshtripathicom.files.wordpress.com/2021/06/image-7.png\" width=800> <br>\n","표준정규분포의 속성을 갖도록 피처가 재조정되는 것이다.<br>\n","0 주위에 표준편차 1의 값으로 배치되도록 피처를 표준화하는 것\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 라이브러리 임포트\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 스케일러 호출\n","scaler = StandardScaler()\n","\n","# 스케일링 (Train, Test)\n","X_train_std = scaler.fit_transform(X_train)\n","x_test_std = scaler.transform(X_test)\n","\n","\n","# 다른 방법\n","#scaler.fit(X_train)\n","#X_train = scaler.transform(X_train)\n","#x_test = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["\n","#### __② MinMax Scaler (정규화)__\n","<img src=\"https://ashutoshtripathicom.files.wordpress.com/2021/06/image-3.png\" width=800> <br>\n"," Min-Max Scaling은 모든 피처가 정확하게 [0,1] 사이에 위치하도록 데이터를 변경한다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 라이브러리 임포트\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 스케일러 호출\n","scaler = MinMaxScaler()\n","\n","# 스케일링 (Train, Test)\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["### __<font color=red>[실습 1]</font> 머신러닝 준비하기__\n","\n","#### __<font color='red'>[Q]</font> 'trm_yn'를 y(target)으로 나머지 컬럼을 X(feature)로 만드세요.__\n","target 변수 : y, feature 변수 : X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 여기에 입력하세요.\n","x = df.drop('trm_yn', axis=1)\n","y = df['trm_yn']\n","\n","# X = df.drop(columns=['trm_yn']).values\n","# y = df['trm_yn'].values"]},{"cell_type":"markdown","metadata":{},"source":["#### __<font color='red'>[Q]</font> x와 y 값을 가지고 8:2 비율로 Train, Test 데이터 셋을 나누세요.__\n","train 데이터 셋 : X_train, y_train<br>\n","test 데이터 셋 : X_test, y_test<br>\n","파라미터 : stratify = y, ramdom_state=42<br>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 여기에 입력하세요.\n","X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["#### __<font color='red'>[Q]</font> 사이킷런의 MinMaxScaler를 활용하여 x_train, x_test 데이터를 정규화 하세요.__"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 여기에 입력하세요.\n","scaler = MinMaxScaler()\n","\n","X_train_s = scaler.fit_transform(X_train)\n","X_test_s = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","\n","\n"," \n","\n","### __5. 머신러닝 모델 구현__\n","\n","> ① 단일 분류예측 모델 : LogisticRegression, KNN, DecisionTree <br>\n","> ② 앙상블(Ensemble) 모델 : RandomForest, XGBoost, LGBM, Stacking, Weighted Blending <br>\n","\n","\n","#### __① 단일 분류예측 모델__\n","\n","#### __- 로지스틱 회귀 (LogisticRegression, 분류)__\n","이진 불류 규칙은 0과 1의 두 클래스를 갖는 것으로, 일반 선형 회귀 모델을 이진분류에 사용할 수 없습니다.<br>\n","대신 선형 회귀를 시그모이드 함수를 사용하여 로지스틱 회귀 곡선으로 변환할 수 있으며, <br>\n","  로지스틱 회귀 곡선은 0과 1사이에서만 이동할 수 있어 분류에 사용할 수 있습니다. \n","\n","- __라이브러리 임포트__\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 라이브러리 임포트\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix \n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.metrics import classification_report"]},{"cell_type":"markdown","metadata":{},"source":["- __모델 학습하기__"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 모델 호출하기\n","# 파라미터 (C: 규제강도, max_iter: 계산에 사용할 작업 수)\n","lr_model = LogisticRegression()\n","\n","# 모델 학습하기\n","lr_model.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["- __모델 예측하기__"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lr_pred = lr_model.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["- __모델 성능 평가하기__"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 오차행렬 (TN  FP, FN  TP)\n","print(\"1.오차 행렬(confusion_matrix)\")\n","print(confusion_matrix(y_test, lr_pred))\n","\n","# 정확도\n","print(\"\\n2.정확도(acciracy_score)\")\n","print(accuracy_score(y_test, lr_pred) )\n","\n","# 정밀도\n","print(\"\\n3.정밀도(precision_score)\")\n","print(precision_score(y_test, lr_pred) )\n","\n","# 재현율 \n","print(\"\\n3.재현율(recall_score)\")\n","print(recall_score(y_test, lr_pred))  \n","\n","# 정밀도 + 재현율\n","print(\"\\n4.F1 점수\")\n","print(f1_score(y_test, lr_pred) )  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Classification Report\n","print(classification_report(y_test, lr_pred))"]},{"cell_type":"markdown","metadata":{},"source":["\n","#### __② 앙상블(Essemble) 모델__\n","\n","- 배깅 (Bagging): 여러개의 DecisionTree 활용하고 샘플 중복 생성을 통해 결과 도출. RandomForest\n","- 부스팅 (Boosting): 약한 학습기를 순차적으로 학습을 하되, 이전 학습에 대하여 잘못 예측된 데이터에 가중치를 부여해 오차를 보완해 나가는 방식. XGBoost, LGBM\n","\n","![앙상블](https://teddylee777.github.io/images/2019-12-18/image-20191217144823555.png)\n","\n","#### __- 랜덤포레스트(RandomForest)__\n","Bagging 대표적인 모델로써, 훈련셋트를 무작위로 각기 다른 서브셋으로 데이터셋을 만들고<br>\n","여러개의 DecisonTree로 학습하고 다수결로 결정하는 모델\n","\n","**주요 Hyperparameter**\n","- random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것!\n","- n_jobs: CPU 사용 갯수\n","- max_depth: 깊어질 수 있는 최대 깊이. 과대적합 방지용\n","- n_estimators: 앙상블하는 트리의 갯수\n","- max_features: 최대로 사용할 feature의 갯수. 과대적합 방지용\n","- min_samples_split: 트리가 분할할 때 최소 샘플의 갯수. default=2. 과대적합 방지용\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 라이브러리 임포트\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# 모델 불러오기\n","rfc_model = RandomForestClassifier(n_estimators=100)\n","\n","# 모델 학습하기\n","rfc_model.fit(X_train, y_train)\n","\n","# 예측하기\n","rfc_pred = rfc_model.predict(X_test)\n","\n","# 성능평가하기\n","print(classification_report(y_test, rfc_pred))"]},{"cell_type":"markdown","metadata":{},"source":["###  \n","#### __- XGBoost__\n","여러개의 DecisionTree를 결합하여 Strong Learner 만드는 Boosting 앙상블 기법<br>\n","Kaggle 대회에서 자주 사용하는 모델이다.\n","\n","**주요 특징**\n","- scikit-learn 패키지가 아닙니다.\n","- 성능이 우수함\n","- 학습시간이 매우 느리다\n","\n","**주요 Hyperparameter**\n","- random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것!\n","- n_jobs: CPU 사용 갯수\n","- learning_rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1\n","- n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100\n","- max_depth: 트리의 깊이. 과대적합 방지용. default=3. \n","- subsample: 샘플 사용 비율. 과대적합 방지용. default=1.0\n","- max_features: 최대로 사용할 feature의 비율. 과대적합 방지용. default=1.0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 패키지 설치\n","!pip install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 라이브러리 임포트\n","from xgboost import XGBClassifier\n","\n","# 모델 불러오기\n","xgb_model = XGBClassifier(n_estimators=3, random_state=42)  \n","\n","# 모델 학습하기\n","xgb_model.fit(X_train, y_train)\n","\n","# 예측하기\n","xgb_pred = xgb_model.predict(X_test)\n","\n","# 성능평가하기\n","print(classification_report(y_test, xgb_pred))"]},{"cell_type":"markdown","metadata":{},"source":["### __<font color=red>[실습 2]</font> 머신러닝 모델 구현하기__\n","\n","#### __<font color='red'>[Q]</font> 아래의 조건으로 KNN(K-Nearest Neighbor) 모델을 구현해 보세요.__\n","- 모델은 변수명 knn_model 로 저장하세요.\n","- 파라미터 n-neighbors는 5로 설정하세요.\n","- 예측 값은 변수명 knn_pred로 저장하세요.\n","- 성능 평가는 F1-Score로 하세요.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 여기에 입력하세요.\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","knn_model = KNeighborsClassifier(n_neighbors=5)\n","knn_model.fit(X_train_s, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["knn_pred = knn_model.predict(X_test_s)\n","\n","print('F1 Score: ', f1_score(y_test, knn_pred))"]},{"cell_type":"markdown","metadata":{},"source":["#### __<font color='red'>[Q]</font> 아래의 조건으로 결정트리(DecisionTree) 모델을 구현해 보세요.__\n","\n","- 모델은 변수명 dt_model 로 저장하세요.\n","- 파라미터 max_depth는 10로 설정하세요.\n","- 예측 값은 변수명 dt_pred로 저장하세요.\n","- 성능 평가는 정확도로 하세요."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 여기에 입력하세요.\n","from sklearn.tree import DecisionTreeClassifier\n","\n","dt_model = DecisionTreeClassifier(max_depth=10)\n","dt_model.fit(X_train_s, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dt_pred = dt_model.predict(X_test_s)\n","\n","print('Accuracy: ', accuracy_score(y_test, dt_pred))"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","#### __<font color='red'>[Q]</font> 아래의 조건으로 랜덤포레스트(RandomForest) 모델을 구현해 보세요.__\n","- 모델은 변수명 rfc_model 로 저장하세요.\n","- 파라미터 max_depth는 30, n_estimators는 5로 설정하세요.\n","- 예측 값은 변수명 rfc_pred로 저장하세요.\n","- 성능 평가는 classification report 하세요.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 여기에 입력하세요.\n","from sklearn.ensemble import RandomForestClassifier\n","\n","rfc_model = RandomForestClassifier(max_depth=30, n_estimators=5)\n","rfc_model.fit(X_train_s, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rfc_pred = rfc_model.predict(X_test_s)\n","\n","print(classification_report(y_test, rfc_pred))"]},{"cell_type":"markdown","metadata":{},"source":["\n","### __6. 머신러닝 모델 성능 비교하기__\n","\n","#### __① 모델 성능 비교하기__\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 모델 별 accuracy, precission , recall, f1_score를 List 로 저장\n","perfomance_acc = [accuracy_score(y_test, lr_pred), accuracy_score(y_test, knn_pred), accuracy_score(y_test, dt_pred), accuracy_score(y_test, rfc_pred)]\n","perfomance_precision = [precision_score(y_test, lr_pred), precision_score(y_test, knn_pred), precision_score(y_test, dt_pred), precision_score(y_test, rfc_pred)]\n","perfomance_recall = [recall_score(y_test, lr_pred), recall_score(y_test, knn_pred), recall_score(y_test, dt_pred), recall_score(y_test, rfc_pred)]\n","perfomance_f1_score = [f1_score(y_test, lr_pred), f1_score(y_test, knn_pred), f1_score(y_test, dt_pred), f1_score(y_test, rfc_pred)]\n","\n","performance_index = ['accuary', 'precission', 'recall', 'f1_score']\n","performance_columns = ['Logistic', 'KNN', 'Decision', 'RandomForest']\n","\n","# 모델 별 전체 성능을 데이터프레임으로 저장ㅌ\n","performance_total= pd.DataFrame(data=[perfomance_acc, perfomance_precision, perfomance_recall, perfomance_f1_score], index=performance_index, columns=performance_columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["performance_total"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(8,4))\n","plt.title('Accuracy')\n","sns.barplot(x=perfomance_acc, y=performance_columns)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["\n","#### __② 모델 Feature 중요도 비교하기__\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature = df.drop(columns=['trm_yn'])\n","\n","rfc_importances_values = rfc_model.feature_importances_\n","rfc_importances = pd.Series(rfc_importances_values, index=feature.columns )\n","rfc_top10 = rfc_importances.sort_values(ascending=False)[:10]\n","\n","plt.figure(figsize=(8,6))\n","plt.title('Top 10 Feature Importances')\n","sns.barplot(x=rfc_top10, y = rfc_top10.index)\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"9b951021aee1af8a38b85369796df8999da4b77ce8cdb748a95567db2c8a2d83"}}},"nbformat":4,"nbformat_minor":2}
