{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 보안 문제로 출력 부분 삭제 후 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 안녕하세요^^ \n",
    "# AIVLE 미니 프로젝트에 오신 여러분을 환영합니다.\n",
    "* 본 과정에서는 실제 사례와 데이터를 기반으로 문제를 해결하는 전체 과정을 자기 주도형 실습으로 진행해볼 예정입니다.\n",
    "* 앞선 교육과정을 정리하는 마음과 지금까지 배운 내용을 바탕으로 문제 해결을 해볼게요!\n",
    "* 미니 프로젝트를 통한 문제 해결 과정 'A에서 Z까지', 지금부터 시작합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 정의\n",
    "> 문자 메시지 스팸 분류 문제<br>\n",
    "> 1. 문자 메시지 데이터 분석\n",
    "> 2. 스팸 분류 모델 성능 평가\n",
    "### 학습 데이터\n",
    "> * train/validation : spam.csv\n",
    "> * test : spam_test_text.csv\n",
    "### 변수 소개\n",
    "> * text : 문자 메시지\n",
    "> * label : 스팸여부\n",
    "\n",
    "### References\n",
    "> * 한국어 처리\n",
    ">> * [konlpy - 한국어 처리 라이브러리](https://konlpy.org/ko/latest/)\n",
    ">> * [한국어 품사 태그 비교표](https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0)\n",
    ">> * [한국어 품사 태깅 성능 비교](https://konlpy.org/ko/latest/morph/#comparison-between-pos-tagging-classes)\n",
    ">> * [한국어 시스템 사전](https://konlpy.org/ko/latest/data/#corpora)\n",
    "\n",
    "> * 자연어 처리\n",
    ">> * [NLTK](https://www.nltk.org/book/)\n",
    ">> * [gensim](https://radimrehurek.com/gensim/)\n",
    ">> * [Google guide](https://developers.google.com/machine-learning/guides/text-classification/step-2)\n",
    ">> * [WordCloud](https://amueller.github.io/word_cloud/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 탐색부터 먼저 시작해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 라이브러리 설치 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리부터 설치할께요.\n",
    "# !pip install konlpy pandas seaborn gensim wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "fm.findSystemFonts()\n",
    "plt.rcParams['font.family']= [\"Malgun Gothic\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"]=False\n",
    "\n",
    "# GPU 환경 설정하기\n",
    "# assert tf.test.is_gpu_available() == True, 'GPU 설정을 확인하세요.'\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.config.list_logical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 준비\n",
    "#### 1-1. 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 가져옵니다.\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/spam.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head 함수를 이용해 데이터를 확인해봅니다. (띄어쓰기가 되어 있지 않음을 알 수 있어요.)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info() 함수를 이용해서 데이터의 정보를 확인합니다.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 데이터 분포를 확인합니다.\n",
    "data['label'].value_counts(normalize=True).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe 함수를 이용해서 기본 정보를 확인합니다.\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3. 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치를 제거한 후 확인합니다.\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 텍스트 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. 텍스트 길이 분포 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 길이 분포와 최대 길이를 확인합니다.\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 text를 list로 변환\n",
    "text = ' '.join(data.text.explode())\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_spam = nltk.Text(' '.join(data.text.explode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. 형태소/명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlpy를 활용하여 태깅 클래스를 불러옵니다.\n",
    "# 태깅 클래스를 활용하여 형태소/명사를 추출합니다.\n",
    "import nltk\n",
    "from eunjeon import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "spam_morphs = mecab.morphs(text)\n",
    "spam_nouns = mecab.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Samples / Number of words per sample 확인합니다.\n",
    "# https://developers.google.com/machine-learning/guides/text-classification/step-2-5 참고\n",
    "\n",
    "print('형태소 개수:', len(spam_morphs))\n",
    "print(spam_morphs[:10])\n",
    "print()\n",
    "print('명사 개수:', len(spam_nouns))\n",
    "print(spam_nouns[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3. NLTK Text로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize한 문자 데이터를 하나의 nltk.Text로 변환합니다.\n",
    "\n",
    "nltk_spam_morphs = nltk.Text(spam_morphs)\n",
    "nltk_spam_nouns = nltk.Text(spam_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-4. Frequency plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소/명사 추출 각각 단어 분포를 확인합니다.\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.title(\"형태소 개수 : \" + str(len(nltk_spam_morphs.tokens)))\n",
    "nltk_spam_morphs.plot(10)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.title(\"명사 개수 : \" + str(len(nltk_spam_nouns.tokens)))\n",
    "nltk_spam_nouns.plot(10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-5. Similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떤 단어(스팸, 주식...)의 유사 단어를 확인합니다.\n",
    "nltk_spam_nouns.similar('광고')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_spam_nouns.similar('스팸')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_spam_nouns.similar('주식')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-6. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 사전을 확인합니다.\n",
    "nltk_spam_nouns.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_extraction(text):\n",
    "    pos = mecab.pos(text)\n",
    "    extended_list = []\n",
    "    for i in range(len(pos)):\n",
    "        #명사 : NP, NNB, NNP, NNG\n",
    "        #특수기호 : SF, SY\n",
    "        if pos[i][1] in('NP','NNB','NNP','NNG','SF','SY'):\n",
    "            extended_list.append(pos[i][0])\n",
    "    return extended_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = extended_extraction(text)\n",
    "token_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-7. Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연어(collocation)들을 확인합니다.\n",
    "nltk_spam_morphs.collocations(window_size=5)\n",
    "print(\"*\"*50)\n",
    "nltk_spam_nouns.collocations(window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as font_manager\n",
    "from wordcloud import WordCloud\n",
    "NGC_path = font_manager.findfont('Malgun Gothic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1. Morphs vs Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소와 명사 추출 각각에 대해 wordcloud로 확인합니다.\n",
    "# 형태소 추출\n",
    "wc = WordCloud(font_path=NGC_path, max_font_size=40, background_color=\"white\", collocations=False).generate(' '.join(nltk_spam_morphs))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사 추출\n",
    "wc = WordCloud(font_path=NGC_path, max_font_size=40, background_color=\"white\", collocations=False).generate(' '.join(nltk_spam_nouns))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. Ham vs Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사 추출한 결과를 Ham과 Spam에 대해 wordcloud로 확인합니다.\n",
    "ham = data.loc[data['label']=='ham']\n",
    "spam = data.loc[data['label']=='spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_text = ' '.join(ham.text.explode())\n",
    "spam_text = ' '.join(spam.text.explode())\n",
    "\n",
    "ha_only_nouns = mecab.nouns(ham_text)\n",
    "sp_only_nouns = mecab.nouns(spam_text)\n",
    "\n",
    "nltk_ha_nouns = nltk.Text(ha_only_nouns)\n",
    "nltk_sp_nouns = nltk.Text(sp_only_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ham 명사 추출\n",
    "wc = WordCloud(font_path=NGC_path, max_font_size=40, background_color=\"white\", collocations=False).generate(' '.join(nltk_ha_nouns))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam 명사 추출\n",
    "wc = WordCloud(font_path=NGC_path, max_font_size=40, background_color=\"white\", collocations=False).generate(' '.join(nltk_sp_nouns))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23c67d85f209742c672c1b7fd95247539f09df0e36c0a91a355481fa2e35a8db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
