{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 안녕하세요^^ \n",
    "# AIVLE 미니 프로젝트에 오신 여러분을 환영합니다.\n",
    "* 본 과정에서는 실제 사례와 데이터를 기반으로 문제를 해결하는 전체 과정을 자기 주도형 실습으로 진행해볼 예정입니다.\n",
    "* 앞선 교육과정을 정리하는 마음과 지금까지 배운 내용을 바탕으로 문제 해결을 해볼게요!\n",
    "* 미니 프로젝트를 통한 문제 해결 과정 'A에서 Z까지', 지금부터 시작합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "### reference\n",
    "> * [Google guide](https://developers.google.com/machine-learning/guides/text-classification/step-3)\n",
    "> * N-grams\n",
    ">> * [scikit-learn working with text data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#)\n",
    ">> * [scikit-learn text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    ">> * [한글 자료](https://datascienceschool.net/03%20machine%20learning/03.01.03%20Scikit-Learn%EC%9D%98%20%EB%AC%B8%EC%84%9C%20%EC%A0%84%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%8A%A5.html)\n",
    "> * Sequence\n",
    ">> * [keras text classification](https://keras.io/examples/nlp/text_classification_from_scratch/)\n",
    ">> * [tensorflow text classification](https://www.tensorflow.org/tutorials/keras/text_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 라이브러리 설치 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "## import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#fm.findSystemFonts()\n",
    "plt.rcParams['font.family']= [\"Malgun Gothic\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"]=False\n",
    "\n",
    "# GPU 환경 설정하기\n",
    "# assert tf.test.is_gpu_available() == True, 'GPU 설정을 확인하세요.'\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.config.list_logical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 가져옵니다.\n",
    "data = pd.read_csv('./data/spam.csv')\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. processing label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 데이터를 수치형으로 변환합니다.\n",
    "data['label'].loc[data['label']=='ham'] = 0\n",
    "data['label'].loc[data['label']=='spam'] = 1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['text']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train Validation(Test) Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train validation set으로 분리합니다.\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(x, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = pd.read_csv('./data/spam_test_text.csv')['text']\n",
    "Y_te = pd.read_csv('./data/spam_test_label.csv')['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vectorize texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1. N-grams Vectorize [참고](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#extracting-features-from-text-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16071, 29532)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Count Vectorize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer(min_df=1)\n",
    "x_train_c = count_vec.fit_transform(X_tr)\n",
    "x_val_c = count_vec.transform(X_val)\n",
    "x_te_c = count_vec.transform(X_te)\n",
    "x_train_c.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_mecab_vec = CountVectorizer(tokenizer=mecab.morphs, min_df=1)\n",
    "# x_train_mecab_c = count_mecab_vec.fit_transform(X_tr)\n",
    "# x_val_mecab_c = count_mecab_vec.transform(X_val)\n",
    "# x_te_mecab_c = count_mecab_vec.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16071, 29532)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Tf-idf Transform\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=True)\n",
    "x_train_tfidf = transformer.fit_transform(x_train_c)\n",
    "x_val_tfidf = transformer.transform(x_val_c)\n",
    "x_te_tfidf = transformer.transform(x_te_c)\n",
    "x_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vec = TfidfVectorizer(tokenizer=mecab.morphs)\n",
    "# x_train_tfidfv = tfidf_vec.fit_transform(X_tr)\n",
    "# x_val_tfidfv = tfidf_vec.transform(X_val)\n",
    "# x_te_tfidfv = tfidf_vec.transform(X_te)\n",
    "# x_train_tfidfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer = CountVectorizer + TfidfTransformer (but 완벽하게 같지는 않음) \n",
    "# (x_train_tfidf != x_train_tfidfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Count Vectorizer Vocabulary size: ', len(count_vec.vocabulary_))\n",
    "# print('Count Vectorizer(Mecab tokenizer) Vocabulary size: ', len(count_mecab_vec.vocabulary_))\n",
    "# print('TF-IDF Vectorizer(Mecab tokenizer) Vocabulary size: ', len(tfidf_vec.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 'k' of the vectorized features\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "TOP_K = 20000\n",
    "selector = SelectKBest(f_classif, k=min(TOP_K, x_train_tfidf.shape[1]))\n",
    "selector.fit(x_train_tfidf, Y_tr)\n",
    "x_train_ngram = selector.transform(x_train_tfidf).astype('float32')\n",
    "x_val_ngram = selector.transform(x_val_tfidf).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16071, 20000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_ngram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. Sequence Vectorize [참고](https://developers.google.com/machine-learning/guides/text-classification/step-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "TOP_K = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "X_mor_tr = X_tr.apply(lambda x:' '.join(mecab.morphs(x)))\n",
    "X_mor_val = X_val.apply(lambda x:' '.join(mecab.morphs(x)))\n",
    "X_mor_te = X_te.apply(lambda x:' '.join(mecab.morphs(x)))\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=TOP_K, char_level=False)\n",
    "tokenizer.fit_on_texts(X_mor_tr)\n",
    "\n",
    "X_mor_tr_seq = tokenizer.texts_to_sequences(X_mor_tr)\n",
    "X_mor_val_seq = tokenizer.texts_to_sequences(X_mor_val)\n",
    "X_mor_te_seq = tokenizer.texts_to_sequences(X_mor_te)\n",
    "\n",
    "max_length = len(max(X_mor_tr_seq, key=len))\n",
    "if max_length > MAX_SEQUENCE_LENGTH:\n",
    "    max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "print(max_length)\n",
    "\n",
    "X_mor_tr_seq = sequence.pad_sequences(X_mor_tr)\n",
    "X_mor_val_seq = sequence.pad_sequences(X_mor_val)\n",
    "X_mor_te_seq = sequence.pad_sequences(X_mor_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1. Save N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train seq shape: (16071, 20000)\n",
      "X_val seq shape: (4018, 20000)\n"
     ]
    }
   ],
   "source": [
    "# N-grams 방식으로 vectorize한 데이터의 shape을 확인해봅니다.\n",
    "import numpy as np\n",
    "\n",
    "print('X_train tfidf shape:', x_train_tfidf.shape)\n",
    "print('X_val tfidf shape:', x_val_tfidf.shape)\n",
    "print('X_te tfidf shape:', x_te_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습시에 활용 가능하도록 전처리 데이터를 저장해보도록 하겠습니다.\n",
    "import scipy.sparse\n",
    "\n",
    "scipy.sparse.save_npz('X_tfidf_train', x_train_tfidf)\n",
    "scipy.sparse.save_npz('X_tfidf_val', x_val_tfidf)\n",
    "scipy.sparse.save_npz('X_tfidf_te', x_te_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2. Save sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train seq shape: (16071, 20000)\n",
      "X_val seq shape: (4018, 20000)\n"
     ]
    }
   ],
   "source": [
    "print('X_mor_train seq shape:', X_mor_tr_seq.shape)\n",
    "print('X_mor_val seq shape:', X_mor_val_seq.shape)\n",
    "print('X_mor_te seq shape:', X_mor_te_seq.shape)\n",
    "\n",
    "np.save('X_mor_sequence_train', arr=X_mor_tr_seq)\n",
    "np.save('X_mor_sequence_val', arr=X_mor_val_seq)\n",
    "np.save('X_mor_sequence_te', arr=X_mor_te_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3. Save label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train seq shape: (16071,)\n",
      "Y_val seq shape: (4018,)\n"
     ]
    }
   ],
   "source": [
    "# label 데이터의 shape을 확인하고 저장합니다.\n",
    "print('Y_train seq shape:', Y_tr.shape)\n",
    "print('Y_val seq shape:', Y_val.shape)\n",
    "print('Y_te seq shape:', Y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_train', arr=Y_tr)\n",
    "np.save('y_val', arr=Y_val)\n",
    "np.save('y_te', arr=Y_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23c67d85f209742c672c1b7fd95247539f09df0e36c0a91a355481fa2e35a8db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
